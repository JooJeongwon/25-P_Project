import logging  # 1. ë¡œê¹… ëª¨ë“ˆ ì„í¬íŠ¸ (í•„ìˆ˜)
import traceback

from fastapi import FastAPI
from pydantic import BaseModel
from app.crawler import crawl_product

# 2. ë¡œê±° ì„¤ì • (ì´ ë¶€ë¶„ì´ ë¹ ì ¸ì„œ ì—ëŸ¬ê°€ ë‚œ ê²ë‹ˆë‹¤)
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI()

class CrawlRequest(BaseModel):
    url: str
    max_pages: int = 5

@app.post("/crawl")
def crawl(req: CrawlRequest):
    # 3. ë¡œê·¸ ì¶œë ¥
    logger.info(f"ğŸ‘‰ [REQUEST] URL: {req.url}") 
    
    try:
        result = crawl_product(req.url, req.max_pages)
        logger.info(f"ğŸ‘ˆ [RESULT] Data: {result}") # ê²°ê³¼ í™•ì¸ìš©
        return result
    except Exception as e:
        print(f"âŒ í¬ë¡¤ë§ ì—ëŸ¬ ë°œìƒ: {str(e)}")
        traceback.print_exc()
        logger.error(f"âŒ [ERROR] During crawling: {e}")
        raise e